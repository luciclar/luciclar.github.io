---
title: Physics-Informed Neural Networks for SPDEs
description: "Work with Hugo Gangloff, Nicolas Jouvin, Antoine Regardin (M2 student)"
author:
  - name: Lucia Clarotto
    #url: https://samanthacsik.github.io/
    #orcid: 0000-0002-5300-3075
    #affiliation: Master of Environmental Data Science Program @ The Bren School (UCSB) & The National Center for Ecological Analysis & Synthesis
    #affiliation-url: https://ucsb-meds.github.io/ 
#date: 10-24-2022
#citation: 
  #url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ 
image: pinns.jpg
categories: [pinns, spde, generative models] 
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
bibliography: ref_pinns_spde.bib
---

## Physics-Informed Neural Networks for SPDEs

Spatio-temporal data are prevalent in many applications within modern ecology and climate science. Statistical modeling of such data presents a significant challenge, historically focusing on Gaussian random fields (GRFs) and kriging for prediction . In a seminal paper, @lindgren2011explicit proposed an inference methodology based on the fact that certain GRFs can be expressed as solutions to stochastic partial differential equations (SPDEs). The most famous example is the spatial Matérn GRFs, which are solutions to the diffusion-type equation:

\begin{equation}
(\kappa^2-\Delta)^{\alpha/2} u = \mathcal{W},
\end{equation}

where $\mathcal{W}$ is a stochastic forcing term (e.g., white noise), and $\theta = (\kappa, \alpha)$ are the model parameters, related to the Matérn covariance function. This approach bridges the gap between physical and statistical modeling, leading to a wealth of research refining the Equation above to model a broader class of random fields, and developing statistical inference procedures to estimate the model parameters. Most of these methods rely on a mesh-based approach, using finite elements or volumes to discretize the equation over a finite set of basis functions. On the other hand, physics-informed neural networks (PINNs, @raissi2019physics) have recently been introduced to solve partial differential equations $\mathcal{N}[u] = 0$, where $\mathcal{N}$ is an arbitrary differential operator. The goal is to find the best neural network $u_\nu$ representing the solution by minimizing its PDE residuals computed at randomly sampled collocation points. This mesh-free approach has proven useful in various contexts and can be extended to inverse problems where the objective is to learn the parameters $\theta$ of the differential operator from certain observations of the solution.

This work aims to generalize the PINN approach to solve SPDEs. To do so, several modifications to the deterministic framework are necessary and will need to be explored. First, the neural network must represent a stochastic process, which can be achieved through generative modeling where the network $u_\nu(Z)$ has an additional latent variable $Z\sim \mathbb{Q}$ as input. The quantity of interest then becomes the distribution $\mathbb{P}_\nu$ of the network’s SPDE residuals $\mathcal{N}[u_\nu](Z)$. This can be interpreted as a generative model, where $\mathbb{P}_\nu := \mathcal{N}[u_\nu](\cdot) \# \mathbb{Q}$ is the pushforward of the base distribution of $Z$ through the PINN.

Second, it is necessary to define an appropriate loss function that accounts for the stochastic nature of the objects involved. Since SPDEs prescribe equality in distribution, a natural choice is to consider a measure of similarity between the probability distributions $D(\mathbb{P}_\nu, \mathcal{W})$. Several options are possible, such as Kullback-Leibler divergence, $p$-Wasserstein distance as in Arjovsky et al. (2017), or maximum mean discrepancy associated with a certain reproducing kernel [@gretton2012kernel], each leading to different learning strategies for the network parameters $\nu$.

The goal is to explore and implement the various methodologies discussed above.

### First step : Antoine Regardin's internship

A first step in this direction has been made by the M2 student Antoine Regardin during a 6-month internship in 2025. 

He focused on one dimensional SPDEs and integrated randomness into the PINN framework through a Polynomial Chaos Expansion for the solution of the SPDE and a Karhunen-Loève expansion for the stochastic forcing random field.

He prepared the ground for generative models based on probability distribution metrics and weak form of the problems.

---

::: {#refs}
:::