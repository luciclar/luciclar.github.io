@article{clarotto2024,
title = {The {SPDE} approach for spatio-temporal datasets with advection and diffusion},
journal = {Spatial Statistics},
year = {2024},
volume = {62},
pages = {100847},
issn = {2211-6753},
doi = {https://doi.org/10.1016/j.spasta.2024.100847},
url = {https://www.sciencedirect.com/science/article/pii/S2211675324000381},
author = {Lucia Clarotto and Denis Allard and Thomas Romary and Nicolas Desassis}
}


@article{wikle2023statistical,
   author = "Wikle, Christopher K. and Zammit-Mangion, Andrew",
   title = "Statistical Deep Learning for Spatial and Spatiotemporal Data", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2023",
   volume = "10",
   pages = "247-270",
   doi = "https://doi.org/10.1146/annurev-statistics-033021-112628",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-033021-112628",
   publisher = "Annual Reviews",
   issn = "2326-831X"
  }

@article{walchessen2024neural,
title = {Neural likelihood surfaces for spatial processes with computationally intensive or intractable likelihoods},
journal = {Spatial Statistics},
volume = {62},
pages = {100848},
year = {2024},
issn = {2211-6753},
doi = {https://doi.org/10.1016/j.spasta.2024.100848},
url = {https://www.sciencedirect.com/science/article/pii/S2211675324000393},
author = {Julia Walchessen and Amanda Lenzi and Mikael Kuusela}
}

@article{sainsbury2023neural,
  title={Neural Bayes estimators for irregular spatial data using graph neural networks},
  author={Sainsbury-Dale, Matthew and Richards, Jordan and Zammit-Mangion, Andrew and Huser, Rapha{\"e}l},
  journal={arXiv preprint arXiv:2310.02600},
  year={2023}
}


@article{zammitmangion2025neural,
   author = "Zammit-Mangion, Andrew and Sainsbury-Dale, Matthew and Huser, Raphaël",
   title = "Neural Methods for Amortized Inference", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2025",
   volume = "12",
   pages = "311-335",
   doi = "https://doi.org/10.1146/annurev-statistics-112723-034123",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-112723-034123",
   publisher = "Annual Reviews",
   issn = "2326-831X"
}



@article{lenzi2023neural,
  title={Neural networks for parameter estimation in intractable models},
  author={Lenzi, Amanda and Bessac, Julie and Rudi, Johann and Stein, Michael L},
  journal={Computational Statistics \& Data Analysis},
  volume={185},
  pages={107762},
  year={2023},
  publisher={Elsevier}
}

@article{gerber202fast,
author = {Gerber, Florian and Nychka, Douglas},
title = {Fast covariance parameter estimation of spatial Gaussian process models using neural networks},
journal = {Stat},
volume = {10},
number = {1},
pages = {e382},
keywords = {geostatistics, image analysis, neural networks, spatial statistics, statistical computing},
doi = {https://doi.org/10.1002/sta4.382},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.382},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.382},
abstract = {Gaussian processes (GPs) are a popular model for spatially referenced data and allow descriptive statements, predictions at new locations, and simulation of new fields. Often, a few parameters are sufficient to parameterize the covariance function, and maximum likelihood (ML) methods can be used to estimate these parameters from data. ML methods, however, are computationally demanding. For example, in the case of local likelihood estimation, even fitting covariance models on modest size windows can overwhelm typical computational resources for data analysis. This limitation motivates the idea of using neural network (NN) methods to approximate ML estimates. We train NNs to take moderate size spatial fields or variograms as input and return the range and noise-to-signal covariance parameters. Once trained, the NNs provide estimates with a similar accuracy compared to ML estimation and at a speedup by a factor of 100 or more. Although we focus on a specific covariance estimation problem motivated by a climate science application, this work can be easily extended to other, more complex, spatial problems and provides a proof-of-concept for this use of machine learning in computational statistics.},
year = {2021}
}

@misc{jansson2024nonstationarygaussianrandomfields,
      title={Non-stationary Gaussian random fields on hypersurfaces: Sampling and strong error analysis}, 
      author={Erik Jansson and Annika Lang and Mike Pereira},
      year={2024},
      eprint={2406.08185},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2406.08185}, 
}

@misc{gardner2021,
      title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration}, 
      author={Jacob R. Gardner and Geoff Pleiss and David Bindel and Kilian Q. Weinberger and Andrew Gordon Wilson},
      year={2021},
      eprint={1809.11165},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.11165}, 
}

@article{lindgren2011,
author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
title = {{An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach}},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {73},
number = {4},
year = {2011},
pages = {423--498}
}


@article{pereira2019,
title = {{Efficient simulation of Gaussian Markov random fields by Chebyshev polynomial approximation}},
journal = {Spatial Statistics},
volume = {31},
pages = {100359},
year = {2019},
author = {Mike Pereira and Nicolas Desassis},
}

@article{baydin2018,
author = {Baydin, Atilim and Pearlmutter, Barak and Radul, Alexey and Siskind, Jeffrey},
year = {2018},
month = {04},
pages = {1-43},
title = {Automatic differentiation in machine learning: A survey},
volume = {18},
journal = {Journal of Machine Learning Research}
}

@article{cranmer2020frontier,
  title={The frontier of simulation-based inference},
  author={Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30055--30062},
  year={2020},
  publisher={National Academy of Sciences}
}


@InProceedings{oskarsson2022,
  title = 	 {Scalable Deep {G}aussian {M}arkov Random Fields for General Graphs},
  author =       {Oskarsson, Joel and Sid{\'e}n, Per and Lindsten, Fredrik},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {17117--17137},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/oskarsson22a/oskarsson22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/oskarsson22a.html},
  abstract = 	 {Machine learning methods on graphs have proven useful in many applications due to their ability to handle generally structured data. The framework of Gaussian Markov Random Fields (GMRFs) provides a principled way to define Gaussian models on graphs by utilizing their sparsity structure. We propose a flexible GMRF model for general graphs built on the multi-layer structure of Deep GMRFs, originally proposed for lattice graphs only. By designing a new type of layer we enable the model to scale to large graphs. The layer is constructed to allow for efficient training using variational inference and existing software frameworks for Graph Neural Networks. For a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. This allows for making predictions with accompanying uncertainty estimates. The usefulness of the proposed model is verified by experiments on a number of synthetic and real world datasets, where it compares favorably to other both Bayesian and deep learning methods.}
}



@InProceedings{borovitskiy2021,
  title = 	 { Mat{é}rn Gaussian Processes on Graphs },
  author =       {Borovitskiy, Viacheslav and Azangulov, Iskander and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc and Durrande, Nicolas},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2593--2601},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/borovitskiy21a/borovitskiy21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/borovitskiy21a.html},
  abstract = 	 { Gaussian processes are a versatile framework for learning unknown functions in a manner that permits one to utilize prior information about their properties. Although many different Gaussian process models are readily available when the input space is Euclidean, the choice is much more limited for Gaussian processes whose input space is an undirected graph. In this work, we leverage the stochastic partial differential equation characterization of Matérn Gaussian processes—a widely-used model class in the Euclidean setting—to study their analog for undirected graphs. We show that the resulting Gaussian processes inherit various attractive properties of their Euclidean and Riemannian analogs and provide techniques that allow them to be trained using standard methods, such as inducing points. This enables graph Matérn Gaussian processes to be employed in mini-batch and non-conjugate settings, thereby making them more accessible to practitioners and easier to deploy within larger learning frameworks. }
}

